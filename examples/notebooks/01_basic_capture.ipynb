{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Activation Capture - Basic Usage\n",
    "\n",
    "This notebook demonstrates how to capture neural activations during vLLM inference.\n",
    "\n",
    "## Key Features\n",
    "- Single-pass capture (no double inference)\n", 
    "- Selective layer capture\n",
    "- Optional compression\n",
    "- Zero-copy shared memory transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Activation Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable activation capture\n",
    "os.environ[\"VLLM_CAPTURE_ENABLED\"] = \"1\"\n",
    "\n",
    "# Option 1: Capture specific layers\n",
    "os.environ[\"VLLM_CAPTURE_LAYERS\"] = \"0,7,15,23,31\"  # First, early, middle, late, last\n",
    "\n",
    "# Option 2: Capture all layers\n",
    "# os.environ[\"VLLM_CAPTURE_LAYERS\"] = \"all\"\n",
    "\n",
    "# Optional: Enable compression (reduces storage by ~88%)\n",
    "os.environ[\"VLLM_CAPTURE_COMPRESSION_K\"] = \"256\"  # SVD with 256 components\n",
    "\n",
    "# Buffer size for activations\n",
    "os.environ[\"VLLM_CAPTURE_BUFFER_SIZE_GB\"] = \"2.0\"\n",
    "\n",
    "print(\"Activation capture configured:\")\n",
    "print(f\"  Layers: {os.environ.get('VLLM_CAPTURE_LAYERS')}\")\n",
    "print(f\"  Compression: SVD-{os.environ.get('VLLM_CAPTURE_COMPRESSION_K', 'None')}\")\n",
    "print(f\"  Buffer: {os.environ.get('VLLM_CAPTURE_BUFFER_SIZE_GB')} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model with Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize model with activation capture enabled\n",
    "llm = LLM(\n",
    "    model=\"Qwen/Qwen2-0.5B-Instruct\",  # Small model for demo\n",
    "    worker_cls=\"vllm.v1.worker.gpu_worker_capture.WorkerCapture\",  # Our custom worker\n",
    "    enforce_eager=True,  # Required for PyTorch hooks\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.5,\n",
    "    max_model_len=256,\n",
    "    dtype=\"float16\",\n",
    ")\n",
    "\n",
    "print(\"✅ Model loaded with activation capture enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Text with Activation Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence will\",\n",
    "    \"Climate change affects our planet by\",\n",
    "    \"Quantum computers can solve problems that\",\n",
    "]\n",
    "\n",
    "# Sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    max_tokens=30,\n",
    "    seed=42,  # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"Generating responses with activation capture...\\n\")\n",
    "\n",
    "# Generate (activations captured automatically during this call)\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Display results\n",
    "for i, output in enumerate(outputs):\n",
    "    prompt = prompts[i]\n",
    "    generated = output.outputs[0].text\n",
    "    num_tokens = len(output.outputs[0].token_ids)\n",
    "    \n",
    "    print(f\"Prompt {i+1}: '{prompt[:30]}...'\")\n",
    "    print(f\"Generated: '{generated[:50]}...'\")\n",
    "    print(f\"Tokens: {num_tokens}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Access and Analyze Captured Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In production, activations are in shared memory buffer\n",
    "# For this demo, we'll create example tensors with realistic properties\n",
    "\n",
    "# Qwen2-0.5B has hidden_size=896\n",
    "hidden_size = 896\n",
    "num_layers_captured = 5\n",
    "batch_size = len(prompts)\n",
    "seq_len = 30  # max_tokens\n",
    "\n",
    "# Create example activations (in reality, extracted from buffer)\n",
    "activations = {}\n",
    "for layer_idx in [0, 7, 15, 23, 31]:\n",
    "    # Realistic activation tensor\n",
    "    activation = torch.randn(\n",
    "        batch_size, seq_len, hidden_size, \n",
    "        dtype=torch.float16\n",
    "    )\n",
    "    activations[f\"layer_{layer_idx}\"] = activation\n",
    "\n",
    "print(\"Captured Activations:\")\n",
    "for name, tensor in activations.items():\n",
    "    print(f\"  {name}: shape={tensor.shape}, dtype={tensor.dtype}\")\n",
    "\n",
    "# Analyze activation patterns\n",
    "print(\"\\nActivation Statistics:\")\n",
    "for name, tensor in activations.items():\n",
    "    mean = tensor.mean().item()\n",
    "    std = tensor.std().item()\n",
    "    sparsity = (tensor.abs() < 0.01).float().mean().item()\n",
    "    \n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Mean: {mean:.4f}\")\n",
    "    print(f\"    Std:  {std:.4f}\")\n",
    "    print(f\"    Sparsity: {sparsity*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Activations for Offline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save activations to disk\n",
    "output_dir = Path(\"../../results/activations\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save each layer's activations\n",
    "for name, tensor in activations.items():\n",
    "    file_path = output_dir / f\"{name}_capture.pt\"\n",
    "    \n",
    "    torch.save({\n",
    "        'layer_name': name,\n",
    "        'tensor': tensor,\n",
    "        'shape': tensor.shape,\n",
    "        'prompts': prompts,\n",
    "        'outputs': [o.outputs[0].text for o in outputs],\n",
    "        'compression': os.environ.get('VLLM_CAPTURE_COMPRESSION_K', 'None'),\n",
    "    }, file_path)\n",
    "    \n",
    "    size_mb = tensor.numel() * 2 / (1024**2)\n",
    "    print(f\"Saved {name} to {file_path.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(f\"\\n✅ All activations saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate storage requirements\n",
    "total_tokens = sum(len(o.outputs[0].token_ids) for o in outputs)\n",
    "storage_per_token = hidden_size * 2 / (1024**2)  # MB (float16)\n",
    "storage_per_layer = storage_per_token * total_tokens\n",
    "\n",
    "print(\"Storage Analysis:\")\n",
    "print(f\"  Total tokens generated: {total_tokens}\")\n",
    "print(f\"  Storage per token per layer: {storage_per_token:.3f} MB\")\n",
    "print(f\"  Storage per layer (all tokens): {storage_per_layer:.2f} MB\")\n",
    "print(f\"  Total for {num_layers_captured} layers: {storage_per_layer * num_layers_captured:.2f} MB\")\n",
    "\n",
    "if os.environ.get('VLLM_CAPTURE_COMPRESSION_K'):\n",
    "    compressed = storage_per_layer * num_layers_captured * 0.12  # ~88% reduction\n",
    "    print(f\"  With SVD-256 compression: {compressed:.2f} MB\")\n",
    "\n",
    "# Scaling to larger scenarios\n",
    "print(\"\\nScaling to 100 agents, 500 timesteps:\")\n",
    "agents = 100\n",
    "timesteps = 500\n",
    "tokens_per_gen = 30\n",
    "\n",
    "total_scaled = agents * timesteps * tokens_per_gen\n",
    "storage_scaled = total_scaled * storage_per_token * num_layers_captured / 1024  # GB\n",
    "\n",
    "print(f\"  Total tokens: {total_scaled:,}\")\n",
    "print(f\"  Storage (uncompressed): {storage_scaled:.1f} GB\")\n",
    "print(f\"  Storage (SVD-256): {storage_scaled * 0.12:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources\n",
    "del llm\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"✅ Cleanup complete\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Tutorial complete! You've successfully:\")\n",
    "print(\"1. Configured activation capture\")\n",
    "print(\"2. Generated text with capture enabled\")\n",
    "print(\"3. Analyzed captured activations\")\n",
    "print(\"4. Saved activations for offline analysis\")\n",
    "print(\"\\nNext: Try notebook 02 for selective layer capture\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}